---
layout: page           # 또는 post 레이아웃을 쓰고 싶으면 `post`
title: "Attention is All You Need"
permalink: /paper-review/
---

# Attention is All You Need

_Vaswani et al. (2017)_ 에서 제안된 Transformer 아키텍처는…

## Abstract

여기에 간략한 요약을…

## Key Contributions

- **Self-Attention Mechanism**  
  - 입력 시퀀스 내부의 모든 포지션 간 관계를 학습  
- **Positional Encoding**  
  - 순서 정보 보존을 위한 사인·코사인 함수 기반 인코딩  
- …

*(더 많은 섹션: Methodology, Experiments, Discussion 등으로 확장하세요.)*
